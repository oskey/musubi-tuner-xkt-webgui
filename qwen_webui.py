#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen-Image Web UI
ç°ä»£åŒ–çš„Qwen-Imageè®­ç»ƒWebç•Œé¢
"""

import os
import sys
import json
import subprocess
import threading
import time
import webbrowser
from datetime import datetime
from pathlib import Path
from flask import Flask, render_template_string, request, jsonify, send_from_directory, render_template
from werkzeug.utils import secure_filename
import queue
import signal
import logging

app = Flask(__name__)
app.secret_key = 'qwen_webui_secret_key'

# å…¨å±€å˜é‡
log_queue = queue.Queue()
log_history = []  # å­˜å‚¨æ‰€æœ‰æ—¥å¿—çš„æŒä¹…åˆ—è¡¨
current_process = None
process_lock = threading.Lock()

# å‚æ•°è¯´æ˜å’Œå»ºè®®å€¼
PARAM_DESCRIPTIONS = {
    # æ ¸å¿ƒæ¨¡å‹è·¯å¾„
    'dit_path': {
        'description': 'DiT (Diffusion Transformer) æ¨¡å‹è·¯å¾„',
        'suggestion': 'ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒçš„ qwen_image_bf16.safetensors æ¨¡å‹',
        'required': True,
        'type': 'file'
    },
    'vae_path': {
        'description': 'VAE (å˜åˆ†è‡ªç¼–ç å™¨) æ¨¡å‹è·¯å¾„',
        'suggestion': 'ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒçš„ qwen_image_vae.safetensors æ¨¡å‹',
        'required': True,
        'type': 'file'
    },
    'text_encoder_path': {
        'description': 'æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹è·¯å¾„',
        'suggestion': 'ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒçš„ qwen_2.5_vl_7b.safetensors æ¨¡å‹',
        'required': True,
        'type': 'file'
    },
    'dataset_config': {
        'description': 'æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (TOMLæ ¼å¼)',
        'suggestion': 'åŒ…å«è®­ç»ƒæ•°æ®è·¯å¾„å’Œæ ‡æ³¨ä¿¡æ¯çš„é…ç½®æ–‡ä»¶',
        'required': True,
        'type': 'file'
    },
    
    # è®­ç»ƒæ ¸å¿ƒå‚æ•°
    'mixed_precision': {
        'description': 'æ··åˆç²¾åº¦è®­ç»ƒç±»å‹',
        'suggestion': 'bf16 - æ¨èç”¨äºç°ä»£GPUï¼ŒèŠ‚çœæ˜¾å­˜ä¸”ä¿æŒç²¾åº¦',
        'options': ['no', 'fp16', 'bf16'],
        'default': 'bf16'
    },
    'timestep_sampling': {
        'description': 'æ—¶é—´æ­¥é‡‡æ ·ç­–ç•¥',
        'suggestion': 'shift - å®˜æ–¹æ¨èçš„é‡‡æ ·æ–¹æ³•ï¼Œæå‡è®­ç»ƒæ•ˆæœ',
        'options': ['uniform', 'shift'],
        'default': 'shift'
    },
    'weighting_scheme': {
        'description': 'æŸå¤±æƒé‡æ–¹æ¡ˆ',
        'suggestion': 'none - ä½¿ç”¨é»˜è®¤æƒé‡ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯',
        'options': ['none', 'sigma_sqrt', 'logit_normal'],
        'default': 'none'
    },
    'discrete_flow_shift': {
        'description': 'ç¦»æ•£æµåç§»å‚æ•°',
        'suggestion': '2.2 - å®˜æ–¹è°ƒä¼˜çš„æœ€ä½³å€¼ï¼Œå½±å“ç”Ÿæˆè´¨é‡',
        'range': [1.0, 5.0],
        'default': 2.2
    },
    'optimizer_type': {
        'description': 'ä¼˜åŒ–å™¨ç±»å‹',
        'suggestion': 'adamw8bit - 8ä½AdamWï¼Œæ˜¾è‘—èŠ‚çœæ˜¾å­˜',
        'options': ['adamw', 'adamw8bit', 'lion', 'sgd'],
        'default': 'adamw8bit'
    },
    'learning_rate': {
        'description': 'å­¦ä¹ ç‡',
        'suggestion': 'æ¨èåŒºé—´ï¼š2e-4 ~ 5e-05ï¼ˆè§¦å‘å®¹æ˜“ï¼Œæ•ˆæœç«‹ç«¿è§å½±ï¼‰ã€‚æ•°æ®å°‘æ—¶ï¼Œè¿‡é«˜ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿‡ä½åˆéš¾å­¦åˆ°',
        'range': [1e-6, 1e-3],
        'default': 5e-05
    },
    'max_data_loader_n_workers': {
        'description': 'æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°',
        'suggestion': '2 - å¹³è¡¡åŠ è½½é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ï¼Œå¯æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´',
        'range': [0, 8],
        'default': 2
    },
    'train_batch_size': {
        'description': 'è®­ç»ƒæ‰¹æ¬¡å¤§å°',
        'suggestion': 'ä»TOMLé…ç½®æ–‡ä»¶çš„[general]éƒ¨åˆ†è¯»å–ï¼Œå½±å“æ˜¾å­˜ä½¿ç”¨å’Œè®­ç»ƒé€Ÿåº¦',
        'range': [1, 32],
        'default': 1,
        'toml_source': True
    },
    'num_repeats': {
        'description': 'æ•°æ®é‡å¤æ¬¡æ•°',
        'suggestion': 'ä»TOMLæ–‡ä»¶çš„[[datasets]]éƒ¨åˆ†è¯»å–ã€‚æ¨èå€¼20~30ï¼ˆå›¾ç‰‡è¾ƒå°‘æ—¶ï¼‰ï¼Œæ•°æ®å°‘æ—¶æé«˜é‡å¤æ¬¡æ•°å¯æœ‰æ•ˆæå‡è®­ç»ƒæ•ˆæœ',
        'range': [1, 100],
        'default': 5,
        'toml_source': True
    },
    'network_module': {
        'description': 'LoRAç½‘ç»œæ¨¡å—',
        'suggestion': 'networks.lora_qwen_image - ä¸“ä¸ºQwen-Imageä¼˜åŒ–çš„LoRAå®ç°',
        'default': 'networks.lora_qwen_image',
        'readonly': True
    },
    'network_dim': {
        'description': 'LoRAç»´åº¦ (rank)',
        'suggestion': '8 - æ¨èå€¼ã€‚å¦‚æœåªè®­ç»ƒå•ä¸€è§’è‰²/é£æ ¼ï¼Œ32æ›´åˆé€‚',
        'range': [4, 128],
        'default': 8
    },
    'max_train_epochs': {
        'description': 'æœ€å¤§è®­ç»ƒè½®æ•°',
        'suggestion': '8 - æ•°æ®å°‘æ—¶ï¼Œæé«˜è½®æ•°å¯ä»¥æœ‰æ•ˆæå‡è®­ç»ƒæ•ˆæœ',
        'range': [1, 100],
        'default': 8
    },
    'save_every_n_epochs': {
        'description': 'æ¯Nè½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹',
        'suggestion': '1 - æ¯è½®éƒ½ä¿å­˜ï¼Œä¾¿äºé€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹',
        'range': [1, 10],
        'default': 1
    },
    'seed': {
        'description': 'éšæœºç§å­',
        'suggestion': '42 - å›ºå®šç§å­ç¡®ä¿ç»“æœå¯å¤ç°',
        'range': [0, 2147483647],
        'default': 42
    },
    
    # åŠ é€Ÿå™¨é…ç½®
    'num_cpu_threads_per_process': {
        'description': 'æ¯è¿›ç¨‹CPUçº¿ç¨‹æ•°',
        'suggestion': '1 - é¿å…çº¿ç¨‹ç«äº‰ï¼Œæ¨èå€¼',
        'range': [1, 16],
        'default': 1
    },
    
    # ä¼˜åŒ–é€‰é¡¹
    'sdpa': {
        'description': 'å¯ç”¨SDPA (Scaled Dot-Product Attention) ä¼˜åŒ–',
        'suggestion': 'å¯ç”¨ - æ˜¾è‘—æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡',
        'default': True
    },
    'gradient_checkpointing': {
        'description': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
        'suggestion': 'å¯ç”¨ - ä»¥è®¡ç®—æ—¶é—´æ¢å–æ˜¾å­˜èŠ‚çœï¼Œæ¨èå¯ç”¨',
        'default': True
    },
    'persistent_data_loader_workers': {
        'description': 'æŒä¹…åŒ–æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹',
        'suggestion': 'å¯ç”¨ - é¿å…é‡å¤åˆ›å»ºè¿›ç¨‹ï¼Œæå‡æ€§èƒ½',
        'default': True
    },
    
    # æ—¥å¿—é…ç½®
    'logging_dir': {
        'description': 'æ—¥å¿—ç›®å½•è·¯å¾„',
        'suggestion': './logs - å­˜å‚¨è®­ç»ƒæ—¥å¿—å’ŒTensorBoardæ•°æ®çš„ç›®å½•',
        'default': './logs',
        'type': 'directory'
    },
    'log_with': {
        'description': 'æ—¥å¿—è®°å½•å·¥å…·',
        'suggestion': 'tensorboard - å®˜æ–¹æ¨èçš„å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹',
        'options': ['tensorboard', 'wandb', 'all'],
        'default': 'tensorboard'
    },
    
    # è¾“å‡ºé…ç½®
    'output_dir': {
        'description': 'è¾“å‡ºç›®å½•',
        'suggestion': 'è®­ç»ƒç»“æœä¿å­˜è·¯å¾„',
        'required': True,
        'type': 'directory'
    },
    'output_name': {
        'description': 'è¾“å‡ºæ–‡ä»¶åå‰ç¼€',
        'suggestion': 'ç”Ÿæˆçš„LoRAæ¨¡å‹æ–‡ä»¶å',
        'required': True,
        'type': 'string'
    },
    
    # æ–°å¢å‚æ•° - åŸºäºç”¨æˆ·æœ€æ–°é…ç½®
    'network_args': {
        'description': 'LoRAç½‘ç»œé¢å¤–å‚æ•°',
        'suggestion': 'loraplus_lr_ratio=4 - å¦‚æœä½ æƒ³è®©LoRAæ›´å®¹æ˜“è§¦å‘ï¼Œæé«˜è¯¥å€¼ã€‚ä¸ºLoRAçš„Aå’ŒBçŸ©é˜µè®¾ç½®ä¸åŒå­¦ä¹ ç‡æ¯”ä¾‹',
        'default': 'loraplus_lr_ratio=4',
        'type': 'string'
    },
    'fp8_base': {
        'description': 'å¯ç”¨FP8åŸºç¡€æ¨¡å‹é‡åŒ–',
        'suggestion': 'å¯ç”¨ - æ˜¾è‘—èŠ‚çœæ˜¾å­˜ï¼Œé€‚åˆæ˜¾å­˜ä¸è¶³çš„æƒ…å†µï¼Œå¯¹12GBæ˜¾å¡æ¨èå¯ç”¨',
        'default': True
    },
    'fp8_scaled': {
        'description': 'å¯ç”¨FP8ç¼©æ”¾é‡åŒ–',
        'suggestion': 'å¯ç”¨ - è¿›ä¸€æ­¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼Œä¸fp8_baseé…åˆä½¿ç”¨æ•ˆæœæ›´ä½³',
        'default': True
    },
    'blocks_to_swap': {
        'description': 'äº¤æ¢åˆ°CPUçš„Transformerå—æ•°é‡',
        'suggestion': '20 - å°†éƒ¨åˆ†æ¨¡å‹å±‚äº¤æ¢åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜ï¼Œæ•°å€¼è¶Šå¤§èŠ‚çœæ˜¾å­˜è¶Šå¤šä½†é€Ÿåº¦è¶Šæ…¢',
        'range': [0, 50],
        'default': 20
    }
}

# é»˜è®¤é…ç½® - åŸºäºå®˜æ–¹é»˜è®¤å‘½ä»¤è¡Œå‚æ•°
DEFAULT_CONFIG = {
    # ç¯å¢ƒé…ç½®
    'enable_venv': True,
    'venv_python_path': './venv/Scripts/',
    
    # æ ¸å¿ƒæ¨¡å‹è·¯å¾„ (å¿…éœ€å‚æ•°)
    'dit_path': './model/diffusion_models/qwen_image_bf16.safetensors',  # DiTæ¨¡å‹è·¯å¾„
    'vae_path': './model/vae/qwen_image_vae.safetensors',  # VAEæ¨¡å‹è·¯å¾„
    'text_encoder_path': './model/text_encoders/qwen_2.5_vl_7b.safetensors',  # æ–‡æœ¬ç¼–ç å™¨è·¯å¾„
    'dataset_config': './ai_data/datasets/lovemf_config.toml',  # æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
    
    # è¾“å‡ºé…ç½®
    'output_dir': './output',  # è¾“å‡ºç›®å½•
    'output_name': 'lovemf_lora',  # è¾“å‡ºæ–‡ä»¶å
    
    # è®­ç»ƒå‚æ•° - åŸºäºå®˜æ–¹é»˜è®¤å‘½ä»¤è¡Œå‚æ•°
    'mixed_precision': 'bf16',  # æ··åˆç²¾åº¦è®­ç»ƒï¼Œå»ºè®®å€¼ï¼šbf16
    'timestep_sampling': 'shift',  # æ—¶é—´æ­¥é‡‡æ ·æ–¹æ³•ï¼Œå»ºè®®å€¼ï¼šshift
    'weighting_scheme': 'none',  # æƒé‡æ–¹æ¡ˆï¼Œå»ºè®®å€¼ï¼šnone
    'discrete_flow_shift': 2.2,  # ç¦»æ•£æµåç§»ï¼Œå»ºè®®å€¼ï¼š2.2
    'optimizer_type': 'adamw8bit',  # ä¼˜åŒ–å™¨ç±»å‹ï¼Œå»ºè®®å€¼ï¼šadamw8bit (èŠ‚çœæ˜¾å­˜)
    'learning_rate': 5e-05,  # å­¦ä¹ ç‡ï¼Œæ¨èå€¼ï¼š5e-05
    'max_data_loader_n_workers': 2,  # æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°ï¼Œå»ºè®®å€¼ï¼š2
    'train_batch_size': 1,  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œä»TOMLæ–‡ä»¶è¯»å–
    'num_repeats': 5,  # æ•°æ®é‡å¤æ¬¡æ•°ï¼Œä»TOMLæ–‡ä»¶è¯»å–
    'network_module': 'networks.lora_qwen_image',  # ç½‘ç»œæ¨¡å—ï¼Œå›ºå®šå€¼
    'network_dim': 8,  # LoRAç»´åº¦ï¼Œæ¨èå€¼ï¼š8
    'network_args': 'loraplus_lr_ratio=4',  # LoRAç½‘ç»œé¢å¤–å‚æ•°
    'max_train_epochs': 8,  # æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œç”¨æˆ·é…ç½®å€¼ï¼š8
    'save_every_n_epochs': 1,  # æ¯Nè½®ä¿å­˜ä¸€æ¬¡ï¼Œå®˜æ–¹é»˜è®¤å€¼ï¼š1
    'seed': 42,  # éšæœºç§å­ï¼Œå®˜æ–¹é»˜è®¤å€¼ï¼š42
    
    # åŠ é€Ÿå™¨é…ç½®
    'num_cpu_threads_per_process': 1,  # æ¯è¿›ç¨‹CPUçº¿ç¨‹æ•°ï¼Œå»ºè®®å€¼ï¼š1
    
    # ä¼˜åŒ–é€‰é¡¹ (å®˜æ–¹é»˜è®¤å¯ç”¨)
    'sdpa': True,  # å¯ç”¨SDPAæ³¨æ„åŠ›ä¼˜åŒ–ï¼Œå»ºè®®å¯ç”¨
    'gradient_checkpointing': True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜ï¼Œå»ºè®®å¯ç”¨
    'persistent_data_loader_workers': True,  # æŒä¹…åŒ–æ•°æ®åŠ è½½å™¨ï¼Œæå‡æ€§èƒ½ï¼Œå»ºè®®å¯ç”¨
    
    # æ–°å¢FP8å’Œå†…å­˜ä¼˜åŒ–é€‰é¡¹ - åŸºäºç”¨æˆ·æœ€æ–°é…ç½®
    'fp8_base': True,  # å¯ç”¨FP8åŸºç¡€æ¨¡å‹é‡åŒ–ï¼ŒèŠ‚çœæ˜¾å­˜
    'fp8_scaled': True,  # å¯ç”¨FP8ç¼©æ”¾é‡åŒ–ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ˜¾å­˜
    'blocks_to_swap': 20,  # äº¤æ¢åˆ°CPUçš„Transformerå—æ•°é‡ï¼ŒèŠ‚çœæ˜¾å­˜
    
    # ç¼“å­˜é…ç½® (ç”¨äºé¢„å¤„ç†)
    'cache_dir': './ai_data/cache',
    'batch_size': 1,  # ç”¨äºç¼“å­˜æ“ä½œçš„batch_size
    
    # æ—¥å¿—é…ç½®
    'logging_dir': './logs',
    'log_with': 'tensorboard'
}

def log_message(message, level='info'):
    """æ·»åŠ æ—¥å¿—æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = {
        'timestamp': timestamp,
        'level': level,
        'message': message
    }
    log_queue.put(log_entry)
    log_history.append(log_entry)  # åŒæ—¶æ·»åŠ åˆ°æŒä¹…åˆ—è¡¨
    print(f"[{timestamp}] {level.upper()}: {message}")

def run_command(cmd, step_name, config=None):
    """è¿è¡Œå‘½ä»¤å¹¶å®æ—¶è¾“å‡ºæ—¥å¿—"""
    global current_process
    
    log_message(f"å¼€å§‹æ‰§è¡Œ: {step_name}", 'info')
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è™šæ‹Ÿç¯å¢ƒ
    enable_venv = config.get('enable_venv', True) if config else True
    venv_python_path = config.get('venv_python_path', './venv/Scripts/') if config else './venv/Scripts/'
    
    if enable_venv:
        # æ„å»ºè™šæ‹Ÿç¯å¢ƒPythonè·¯å¾„
        if not venv_python_path.endswith(('/', '\\')):
            venv_python_path += '/'
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if venv_python_path.startswith('./'):
            venv_python_path = venv_python_path[2:]  # ç§»é™¤ './'
        venv_python = os.path.join(os.getcwd(), venv_python_path, 'python.exe')
        venv_python = os.path.normpath(venv_python)
        
        log_message(f"å¯ç”¨è™šæ‹Ÿç¯å¢ƒï¼ŒPythonè·¯å¾„: {venv_python}", 'info')
        
        # å¤„ç†ä¸åŒç±»å‹çš„å‘½ä»¤
        if cmd[0] == sys.executable or cmd[0] == 'python':
            # æ›¿æ¢Pythonå‘½ä»¤
            actual_cmd = [venv_python] + cmd[1:]
            log_message(f"ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒPythonæ‰§è¡Œ: {' '.join(actual_cmd)}", 'debug')
        elif cmd[0] == 'accelerate':
            # å¯¹äºaccelerateå‘½ä»¤ï¼Œç›´æ¥ä½¿ç”¨accelerateå¯æ‰§è¡Œæ–‡ä»¶
            venv_accelerate = os.path.join(os.path.dirname(venv_python), 'accelerate.exe')
            if os.path.exists(venv_accelerate):
                actual_cmd = [venv_accelerate] + cmd[1:]
                log_message(f"ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒaccelerateæ‰§è¡Œ: {' '.join(actual_cmd)}", 'debug')
            else:
                # å¦‚æœaccelerate.exeä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨python -m accelerate
                actual_cmd = [venv_python, '-m', 'accelerate'] + cmd[1:]
                log_message(f"ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒPythonæ‰§è¡Œaccelerateæ¨¡å—: {' '.join(actual_cmd)}", 'debug')
        else:
            actual_cmd = cmd
            log_message("éPythonå‘½ä»¤ï¼Œç›´æ¥æ‰§è¡Œ", 'debug')
    else:
        # ä¸ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼Œç›´æ¥æ‰§è¡ŒåŸå‘½ä»¤
        actual_cmd = cmd
        log_message("æœªå¯ç”¨è™šæ‹Ÿç¯å¢ƒï¼Œç›´æ¥æ‰§è¡Œå‘½ä»¤", 'info')
    
    try:
        with process_lock:
            # åœ¨Windowsä¸Šåˆ›å»ºæ–°çš„è¿›ç¨‹ç»„ï¼Œä»¥ä¾¿èƒ½å¤Ÿç»ˆæ­¢æ‰€æœ‰å­è¿›ç¨‹
            import subprocess
            if os.name == 'nt':  # Windows
                current_process = subprocess.Popen(
                    actual_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    encoding='gbk',
                    errors='ignore',
                    bufsize=1,
                    universal_newlines=True,
                    cwd=os.getcwd(),
                    creationflags=subprocess.CREATE_NEW_PROCESS_GROUP
                )
            else:  # Unix/Linux
                current_process = subprocess.Popen(
                    actual_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    encoding='utf-8',
                    errors='ignore',
                    bufsize=1,
                    universal_newlines=True,
                    cwd=os.getcwd(),
                    preexec_fn=os.setsid
                )
        
        # å®æ—¶è¯»å–è¾“å‡º
        while True:
            output = current_process.stdout.readline()
            if output == '' and current_process.poll() is not None:
                break
            if output:
                log_message(output.strip(), 'output')
        
        return_code = current_process.wait()
        
        # è¿›ç¨‹å®Œæˆåæ¸…é™¤å¼•ç”¨
        with process_lock:
            current_process = None
        
        if return_code == 0:
            log_message(f"âœ… {step_name} å®Œæˆ", 'success')
            return True
        else:
            log_message(f"âŒ {step_name} å¤±è´¥ (è¿”å›ç : {return_code})", 'error')
            return False
            
    except subprocess.TimeoutExpired:
        log_message(f"â° {step_name} æ‰§è¡Œè¶…æ—¶", 'error')
        with process_lock:
            if current_process:
                current_process.kill()
            current_process = None
        return False
    except KeyboardInterrupt:
        log_message(f"â¹ï¸ {step_name} è¢«ç”¨æˆ·ä¸­æ–­", 'warning')
        with process_lock:
            if current_process:
                current_process.terminate()
            current_process = None
        return False
    except Exception as e:
        log_message(f"âŒ {step_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}", 'error')
        with process_lock:
            current_process = None
        return False

def stop_current_process():
    """åœæ­¢å½“å‰è¿è¡Œçš„è¿›ç¨‹"""
    global current_process
    
    with process_lock:
        if current_process and current_process.poll() is None:
            try:
                log_message("æ­£åœ¨åœæ­¢è®­ç»ƒè¿›ç¨‹...", 'warning')
                
                if os.name == 'nt':  # Windows
                    # åœ¨Windowsä¸Šï¼Œå‘é€CTRL_BREAK_EVENTä¿¡å·åˆ°è¿›ç¨‹ç»„
                    try:
                        import signal
                        current_process.send_signal(signal.CTRL_BREAK_EVENT)
                        log_message("å·²å‘é€åœæ­¢ä¿¡å·åˆ°è¿›ç¨‹ç»„", 'info')
                    except Exception as e:
                        log_message(f"å‘é€åœæ­¢ä¿¡å·å¤±è´¥: {str(e)}", 'warning')
                        current_process.terminate()
                else:  # Unix/Linux
                    # åœ¨Unix/Linuxä¸Šï¼Œç»ˆæ­¢æ•´ä¸ªè¿›ç¨‹ç»„
                    import signal
                    os.killpg(os.getpgid(current_process.pid), signal.SIGTERM)
                
                # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œæœ€å¤šç­‰å¾…10ç§’
                try:
                    current_process.wait(timeout=10)
                    log_message("è¿›ç¨‹å·²åœæ­¢", 'info')
                except subprocess.TimeoutExpired:
                    log_message("è¿›ç¨‹æœªåœ¨10ç§’å†…åœæ­¢ï¼Œå¼ºåˆ¶ç»ˆæ­¢", 'warning')
                    
                    if os.name == 'nt':  # Windows
                        # å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ç»„
                        try:
                            import subprocess
                            subprocess.run(['taskkill', '/F', '/T', '/PID', str(current_process.pid)], 
                                         capture_output=True, text=True)
                            log_message("å·²å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ç»„", 'info')
                        except Exception as e:
                            log_message(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ç»„å¤±è´¥: {str(e)}", 'error')
                            current_process.kill()
                    else:  # Unix/Linux
                        os.killpg(os.getpgid(current_process.pid), signal.SIGKILL)
                    
                    current_process.wait()
                    log_message("è¿›ç¨‹å·²å¼ºåˆ¶ç»ˆæ­¢", 'info')
                    
            except Exception as e:
                log_message(f"åœæ­¢è¿›ç¨‹æ—¶å‡ºé”™: {str(e)}", 'error')
                return False
            finally:
                current_process = None
            return True
        else:
            log_message("æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹", 'info')
            return True

def cache_vae_latents(config):
    """é¢„ç¼“å­˜VAE Latents"""
    cmd = [
        'python', 'src/musubi_tuner/qwen_image_cache_latents.py',
        '--dataset_config', config['dataset_config'],
        '--vae', config['vae_path']
    ]
    return run_command(cmd, "é¢„ç¼“å­˜ VAE Latents", config)

def cache_text_encoder_outputs(config):
    """é¢„ç¼“å­˜æ–‡æœ¬ç¼–ç å™¨è¾“å‡º"""
    cmd = [
        'python', 'src/musubi_tuner/qwen_image_cache_text_encoder_outputs.py',
        '--dataset_config', config['dataset_config'],
        '--text_encoder', config['text_encoder_path'],
        '--batch_size', str(config.get('train_batch_size', 1))
    ]
    return run_command(cmd, "é¢„ç¼“å­˜æ–‡æœ¬ç¼–ç å™¨è¾“å‡º", config)

def train_lora(config):
    """LoRAè®­ç»ƒ - åŸºäºå®˜æ–¹é»˜è®¤å‘½ä»¤è¡Œå‚æ•°"""
    # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ—¥å¿—ç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    os.makedirs(config.get('logging_dir', './logs'), exist_ok=True)
    
    # æ„å»ºå®˜æ–¹é»˜è®¤è®­ç»ƒå‘½ä»¤
    # accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 src/musubi_tuner/qwen_image_train_network.py \
    #     --dit path/to/dit_model \
    #     --vae path/to/vae_model \
    #     --text_encoder path/to/text_encoder \
    #     --dataset_config path/to/toml \
    #     --sdpa --mixed_precision bf16 \
    #     --timestep_sampling shift \
    #     --weighting_scheme none --discrete_flow_shift 2.2 \
    #     --optimizer_type adamw8bit --learning_rate 5e-5 --gradient_checkpointing \
    #     --max_data_loader_n_workers 2 --persistent_data_loader_workers \
    #     --network_module networks.lora_qwen_image \
    #     --network_dim 16 \
    #     --max_train_epochs 16 --save_every_n_epochs 1 --seed 42 \
    #     --output_dir path/to/output_dir --output_name name-of-lora
    
    cmd = [
        'accelerate', 'launch',
        '--num_cpu_threads_per_process', str(config.get('num_cpu_threads_per_process', 1)),
        '--mixed_precision', config.get('mixed_precision', 'bf16'),
        'src/musubi_tuner/qwen_image_train_network.py',
        '--dit', config['dit_path'],
        '--vae', config['vae_path'],
        '--text_encoder', config['text_encoder_path'],
        '--dataset_config', config['dataset_config'],
        '--mixed_precision', config.get('mixed_precision', 'bf16'),
        '--timestep_sampling', config.get('timestep_sampling', 'shift'),
        '--weighting_scheme', config.get('weighting_scheme', 'none'),
        '--discrete_flow_shift', str(config.get('discrete_flow_shift', 2.2)),
        '--optimizer_type', config.get('optimizer_type', 'adamw8bit'),
        '--learning_rate', str(config.get('learning_rate', 5e-5)),
        '--max_data_loader_n_workers', str(config.get('max_data_loader_n_workers', 2)),
        '--network_module', config.get('network_module', 'networks.lora_qwen_image'),
        '--network_dim', str(config.get('network_dim', 8)),
        '--network_args', config.get('network_args', 'loraplus_lr_ratio=4'),
        '--max_train_epochs', str(config.get('max_train_epochs', 8)),
        '--save_every_n_epochs', str(config.get('save_every_n_epochs', 1)),
        '--seed', str(config.get('seed', 42)),
        '--logging_dir', config.get('logging_dir', './logs'),
        '--log_with', config.get('log_with', 'tensorboard'),
        '--output_dir', config['output_dir'],
        '--output_name', config['output_name']
    ]
    
    # æ·»åŠ å®˜æ–¹é»˜è®¤çš„å¸ƒå°”é€‰é¡¹
    if config.get('sdpa'):
        cmd.append('--sdpa')
    
    if config.get('gradient_checkpointing'):
        cmd.append('--gradient_checkpointing')
    
    if config.get('persistent_data_loader_workers'):
        cmd.append('--persistent_data_loader_workers')
    
    # æ·»åŠ FP8å’Œå†…å­˜ä¼˜åŒ–é€‰é¡¹
    if config.get('fp8_base', True):
        cmd.append('--fp8_base')
    if config.get('fp8_scaled', True):
        cmd.append('--fp8_scaled')
    if config.get('blocks_to_swap'):
        cmd.extend(['--blocks_to_swap', str(config.get('blocks_to_swap', 20))])
    
    return run_command(cmd, "LoRAè®­ç»ƒ", config)


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/cache_vae', methods=['POST'])
def api_cache_vae():
    # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
    config = {**DEFAULT_CONFIG, **(request.json or {})}
    
    def run_task():
        success = cache_vae_latents(config)
        return success
    
    thread = threading.Thread(target=run_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'å¼€å§‹é¢„ç¼“å­˜ VAE Latents'})

@app.route('/api/cache_text_encoder', methods=['POST'])
def api_cache_text_encoder():
    # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
    config = {**DEFAULT_CONFIG, **(request.json or {})}
    
    def run_task():
        success = cache_text_encoder_outputs(config)
        return success
    
    thread = threading.Thread(target=run_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'å¼€å§‹é¢„ç¼“å­˜æ–‡æœ¬ç¼–ç å™¨è¾“å‡º'})

@app.route('/api/start_training', methods=['POST'])
def api_start_training():
    global current_process
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒè¿›ç¨‹åœ¨è¿è¡Œ
    with process_lock:
        if current_process and current_process.poll() is None:
            return jsonify({'success': False, 'message': 'è®­ç»ƒè¿›ç¨‹å·²åœ¨è¿è¡Œä¸­'})
    
    # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
    config = {**DEFAULT_CONFIG, **(request.json or {})}
    
    def run_task():
        success = train_lora(config)
        return success
    
    thread = threading.Thread(target=run_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'å¼€å§‹ LoRA è®­ç»ƒ'})

@app.route('/api/full_pipeline', methods=['POST'])
def api_full_pipeline():
    # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
    config = {**DEFAULT_CONFIG, **(request.json or {})}
    
    def run_task():
        log_message("ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹", 'info')
        
        # æ­¥éª¤1: é¢„ç¼“å­˜ VAE Latents
        log_message("ğŸ“¦ æ­¥éª¤ 1/3: é¢„ç¼“å­˜ VAE Latents", 'info')
        if not cache_vae_latents(config):
            log_message("âŒ å®Œæ•´æµç¨‹å¤±è´¥: VAE Latents é¢„ç¼“å­˜å¤±è´¥", 'error')
            return False
        
        # æ­¥éª¤2: é¢„ç¼“å­˜æ–‡æœ¬ç¼–ç å™¨è¾“å‡º
        log_message("ğŸ“ æ­¥éª¤ 2/3: é¢„ç¼“å­˜æ–‡æœ¬ç¼–ç å™¨è¾“å‡º", 'info')
        if not cache_text_encoder_outputs(config):
            log_message("âŒ å®Œæ•´æµç¨‹å¤±è´¥: æ–‡æœ¬ç¼–ç å™¨è¾“å‡ºé¢„ç¼“å­˜å¤±è´¥", 'error')
            return False
        
        # æ­¥éª¤3: LoRAè®­ç»ƒ
        log_message("ğŸ¯ æ­¥éª¤ 3/3: LoRA è®­ç»ƒ", 'info')
        # TensorBoardå·²åœ¨Web UIå¯åŠ¨æ—¶è‡ªåŠ¨å¯åŠ¨ï¼Œæ— éœ€é‡å¤å¯åŠ¨
        if not train_lora(config):
            log_message("âŒ å®Œæ•´æµç¨‹å¤±è´¥: LoRA è®­ç»ƒå¤±è´¥", 'error')
            return False
        
        log_message("ğŸ‰ å®Œæ•´è®­ç»ƒæµç¨‹æˆåŠŸå®Œæˆï¼", 'success')
        return True
    
    thread = threading.Thread(target=run_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹'})

@app.route('/api/stop_training', methods=['POST'])
def api_stop_training():
    success = stop_current_process()
    return jsonify({'success': success})

@app.route('/api/start_tensorboard', methods=['POST'])
def api_start_tensorboard():
    """å¯åŠ¨TensorBoard"""
    try:
        start_tensorboard()
        return jsonify({'success': True, 'message': 'TensorBoardå¯åŠ¨ä¸­...'})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/logs', methods=['GET'])
def api_logs():
    # å¤„ç†é˜Ÿåˆ—ä¸­çš„æ–°æ—¥å¿—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    try:
        while True:
            log_entry = log_queue.get_nowait()
            # æ—¥å¿—å·²ç»åœ¨log_messageä¸­æ·»åŠ åˆ°log_historyäº†
    except queue.Empty:
        pass
    
    # è¿”å›æ‰€æœ‰å†å²æ—¥å¿—
    return jsonify({'logs': log_history})



@app.route('/api/save_config', methods=['POST'])
def api_save_config():
    """ä¿å­˜å½“å‰é…ç½®åˆ°JSONæ–‡ä»¶å’ŒTOMLæ–‡ä»¶"""
    try:
        config = request.json
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        log_message(f"æ”¶åˆ°ä¿å­˜é…ç½®è¯·æ±‚ï¼ŒåŒ…å«å‚æ•°: {list(config.keys())}", 'info')
        if 'num_repeats' in config:
            log_message(f"num_repeatså€¼: {config['num_repeats']}", 'info')
        if 'dataset_config' in config:
            log_message(f"dataset_configå€¼: {config['dataset_config']}", 'info')
        if 'network_args' in config:
            log_message(f"network_argså€¼: {config['network_args']}", 'info')
        if 'seed' in config:
            log_message(f"seedå€¼: {config['seed']}", 'info')
        if 'blocks_to_swap' in config:
            log_message(f"blocks_to_swapå€¼: {config['blocks_to_swap']}", 'info')
        
        config_file = Path('./webui_config.json')
        
        # ç‰¹æ®Šå¤„ç†learning_rateï¼Œç¡®ä¿ä»¥ç§‘å­¦è®¡æ•°æ³•æ ¼å¼ä¿å­˜
        if 'learning_rate' in config:
            lr_value = config['learning_rate']
            if isinstance(lr_value, (int, float)) and lr_value == 0.00005:
                config['learning_rate'] = '5e-05'
            elif isinstance(lr_value, str):
                try:
                    lr_float = float(lr_value)
                    if lr_float == 0.00005:
                        config['learning_rate'] = '5e-05'
                except ValueError:
                    pass
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ‰¹æ¬¡å¤§å°å’Œé‡å¤æ¬¡æ•°åˆ°TOMLæ–‡ä»¶
        if 'dataset_config' in config and ('train_batch_size' in config or 'num_repeats' in config):
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
            toml_file = Path(config['dataset_config'])
            toml_content = ""
            
            # å¦‚æœTOMLæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰å†…å®¹
            if toml_file.exists():
                with open(toml_file, 'r', encoding='utf-8') as f:
                    toml_content = f.read()
            
            import re
            updated_params = []
            
            # å¤„ç†batch_size
            if 'train_batch_size' in config:
                batch_size_pattern = r'batch_size\s*=\s*\d+'
                new_batch_size = f"batch_size = {config['train_batch_size']}"
                
                if '[general]' in toml_content:
                    if re.search(batch_size_pattern, toml_content):
                        toml_content = re.sub(batch_size_pattern, new_batch_size, toml_content)
                    else:
                        toml_content = re.sub(r'(\[general\])', r'\1\n' + new_batch_size, toml_content)
                else:
                    if toml_content and not toml_content.endswith('\n'):
                        toml_content += '\n'
                    toml_content += f"\n[general]\n{new_batch_size}\n"
                updated_params.append(f"batch_size={config['train_batch_size']}")
            
            # å¤„ç†num_repeats - ä¿å­˜åˆ°[[datasets]]éƒ¨åˆ†
            if 'num_repeats' in config:
                new_num_repeats = f"num_repeats = {config['num_repeats']}   # æé«˜é‡å¤æ¬¡æ•°ï¼Œå°‘é‡æ•°æ®ä¹Ÿèƒ½æ”¶æ•›"
                
                # æŸ¥æ‰¾[[datasets]]éƒ¨åˆ†åˆ°ä¸‹ä¸€ä¸ªsectionæˆ–æ–‡ä»¶ç»“å°¾
                datasets_pattern = r'(\[\[datasets\]\].*?)(?=\n\[|$)'
                datasets_match = re.search(datasets_pattern, toml_content, re.DOTALL)
                
                if datasets_match:
                    # å¦‚æœ[[datasets]]éƒ¨åˆ†å­˜åœ¨
                    datasets_section = datasets_match.group(1)
                    # åˆ é™¤æ‰€æœ‰ç°æœ‰çš„num_repeatsè¡Œï¼ˆåŒ…æ‹¬æ³¨é‡Šï¼‰
                    lines = datasets_section.split('\n')
                    filtered_lines = []
                    for line in lines:
                        if not re.match(r'^\s*num_repeats\s*=', line.strip()):
                            filtered_lines.append(line)
                    
                    # é‡æ–°æ„å»º[[datasets]]éƒ¨åˆ†ï¼Œåœ¨å¼€å¤´æ·»åŠ num_repeats
                    if filtered_lines and filtered_lines[0].strip() == '[[datasets]]':
                        new_section = filtered_lines[0] + '\n' + new_num_repeats
                        if len(filtered_lines) > 1:
                            new_section += '\n' + '\n'.join(filtered_lines[1:])
                    else:
                        new_section = '\n'.join(filtered_lines) + '\n' + new_num_repeats
                    
                    toml_content = toml_content.replace(datasets_section, new_section)
                else:
                    # å¦‚æœ[[datasets]]éƒ¨åˆ†ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                    if toml_content and not toml_content.endswith('\n'):
                        toml_content += '\n'
                    toml_content += f"\n[[datasets]]\n{new_num_repeats}\n"
                updated_params.append(f"num_repeats={config['num_repeats']}")
            
            # å†™å…¥TOMLæ–‡ä»¶
            with open(toml_file, 'w', encoding='utf-8') as f:
                f.write(toml_content)
            
            if updated_params:
                params_str = ', '.join(updated_params)
                log_message(f"é…ç½®å·²ä¿å­˜åˆ° webui_config.json å’Œ {config['dataset_config']} ({params_str})", 'success')
            else:
                log_message("é…ç½®å·²ä¿å­˜åˆ° webui_config.json", 'success')
        else:
            log_message("é…ç½®å·²ä¿å­˜åˆ° webui_config.json", 'success')
        
        return jsonify({'success': True, 'message': 'é…ç½®ä¿å­˜æˆåŠŸ'})
    except Exception as e:
        log_message(f"ä¿å­˜é…ç½®å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'ä¿å­˜é…ç½®å¤±è´¥: {e}'})

@app.route('/api/load_config', methods=['GET'])
def api_load_config():
    """ä»JSONæ–‡ä»¶å’Œæ•°æ®é›†é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        config_file = Path('./webui_config.json')
        
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # ç‰¹æ®Šå¤„ç†learning_rateï¼Œç¡®ä¿ä»¥ç§‘å­¦è®¡æ•°æ³•æ ¼å¼è¿”å›
            if 'learning_rate' in config:
                lr_value = config['learning_rate']
                if isinstance(lr_value, (int, float)) and lr_value == 0.00005:
                    config['learning_rate'] = '5e-05'
                elif isinstance(lr_value, str):
                    try:
                        lr_float = float(lr_value)
                        if lr_float == 0.00005:
                            config['learning_rate'] = '5e-05'
                    except ValueError:
                        pass
            
            log_message(f"å·²åŠ è½½ä¿å­˜çš„é…ç½® - learning_rate: {config.get('learning_rate', 'N/A')}, network_dim: {config.get('network_dim', 'N/A')}", 'success')
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            config = DEFAULT_CONFIG.copy()
            log_message(f"ä½¿ç”¨é»˜è®¤é…ç½® - learning_rate: {config['learning_rate']}, network_dim: {config['network_dim']}", 'info')
            log_message(f"DEFAULT_CONFIGåŸå§‹å€¼ - learning_rate: {DEFAULT_CONFIG['learning_rate']}, network_dim: {DEFAULT_CONFIG['network_dim']}", 'info')
        
        # å°è¯•ä»TOMLæ–‡ä»¶è¯»å–batch_sizeå’Œnum_repeatsï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        dataset_config_path = None
        if 'dataset_config' in config and config['dataset_config']:
            dataset_config_path = Path(config['dataset_config'])
        else:
            # ä½¿ç”¨é»˜è®¤çš„TOMLæ–‡ä»¶è·¯å¾„
            dataset_config_path = Path(DEFAULT_CONFIG['dataset_config'])
        
        if dataset_config_path and dataset_config_path.exists():
            try:
                import toml
                with open(dataset_config_path, 'r', encoding='utf-8') as f:
                    toml_config = toml.load(f)
                
                # ä»TOMLæ–‡ä»¶ä¸­è¯»å–batch_sizeï¼ˆä»[general]éƒ¨åˆ†ï¼‰
                if 'general' in toml_config and 'batch_size' in toml_config['general']:
                    config['train_batch_size'] = toml_config['general']['batch_size']
                    log_message(f"ä» {dataset_config_path} è¯»å– batch_size: {config['train_batch_size']}", 'info')
                
                # ä»TOMLæ–‡ä»¶ä¸­è¯»å–num_repeatsï¼ˆå…ˆæ£€æŸ¥[general]ï¼Œå†æ£€æŸ¥[[datasets]]ï¼‰
                if 'general' in toml_config and 'num_repeats' in toml_config['general']:
                    config['num_repeats'] = toml_config['general']['num_repeats']
                    log_message(f"ä» {dataset_config_path} [general] è¯»å– num_repeats: {config['num_repeats']}", 'info')
                elif 'datasets' in toml_config and isinstance(toml_config['datasets'], list) and len(toml_config['datasets']) > 0:
                    if 'num_repeats' in toml_config['datasets'][0]:
                        config['num_repeats'] = toml_config['datasets'][0]['num_repeats']
                        log_message(f"ä» {dataset_config_path} [[datasets]] è¯»å– num_repeats: {config['num_repeats']}", 'info')
            except Exception as e:
                log_message(f"è¯»å–æ•°æ®é›†é…ç½®æ–‡ä»¶å¤±è´¥: {e}", 'warning')
        
        return jsonify({'success': True, 'config': config})
    except Exception as e:
        log_message(f"åŠ è½½é…ç½®å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'åŠ è½½é…ç½®å¤±è´¥: {e}', 'config': DEFAULT_CONFIG})

@app.route('/api/test_log', methods=['POST'])
def api_test_log():
    """æ·»åŠ æµ‹è¯•æ—¥å¿—æ¶ˆæ¯"""
    try:
        data = request.get_json()
        message = data.get('message', 'æµ‹è¯•æ—¥å¿—æ¶ˆæ¯')
        log_message(message, 'info')
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/clear_logs', methods=['POST'])
def api_clear_logs():
    """æ¸…ç©ºæ‰€æœ‰æ—¥å¿—å†å²"""
    try:
        global log_history
        log_history.clear()
        # æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„æ—¥å¿—
        while not log_queue.empty():
            try:
                log_queue.get_nowait()
            except queue.Empty:
                break
        return jsonify({'success': True, 'message': 'æ—¥å¿—å·²æ¸…ç©º'})
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/reset_config', methods=['POST'])
def api_reset_config():
    """é‡ç½®é…ç½®ä¸ºé»˜è®¤å€¼ï¼ˆä¿ç•™æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼‰"""
    try:
        config_file = 'webui_config.json'
        current_config = DEFAULT_CONFIG.copy()
        
        # ç¡®ä¿å…³é”®å‚æ•°ä½¿ç”¨æ­£ç¡®çš„é»˜è®¤å€¼ï¼Œlearning_rateä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•æ ¼å¼
        current_config['learning_rate'] = '5e-05'
        current_config['network_dim'] = 8
        
        log_message(f"é‡ç½®é…ç½® - learning_rate: {current_config['learning_rate']}, network_dim: {current_config['network_dim']}", 'info')
        log_message(f"DEFAULT_CONFIGåŸå§‹å€¼ - learning_rate: {DEFAULT_CONFIG['learning_rate']}, network_dim: {DEFAULT_CONFIG['network_dim']}", 'info')
        log_message("é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼", 'success')
        return jsonify(current_config)
    except Exception as e:
        log_message(f"é‡ç½®é…ç½®å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'é‡ç½®é…ç½®å¤±è´¥: {e}'})

@app.route('/api/check_files', methods=['GET'])
def api_check_files():
    """æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    try:
        # æ£€æŸ¥æ•°æ®é›†é…ç½®æ–‡ä»¶
        dataset_exists = os.path.exists('dataset_config.toml')
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆä»é»˜è®¤é…ç½®è·å–è·¯å¾„ï¼‰
        model_path = DEFAULT_CONFIG.get('pretrained_model_name_or_path', '')
        model_exists = os.path.exists(model_path) if model_path else False
        
        # æ£€æŸ¥VAEæ¨¡å‹æ–‡ä»¶
        vae_path = DEFAULT_CONFIG.get('vae', '')
        vae_exists = os.path.exists(vae_path) if vae_path else False
        
        return jsonify({
            'dataset_exists': dataset_exists,
            'model_exists': model_exists,
            'vae_exists': vae_exists
        })
    except Exception as e:
        log_message(f"æ£€æŸ¥æ–‡ä»¶å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'æ£€æŸ¥æ–‡ä»¶å¤±è´¥: {e}'})

@app.route('/api/read_batch_size_from_toml', methods=['POST'])
def api_read_batch_size_from_toml():
    """ä»TOMLæ–‡ä»¶è¯»å–batch_size"""
    try:
        data = request.get_json()
        toml_path = data.get('toml_path')
        
        if not toml_path:
            return jsonify({'success': False, 'message': 'TOMLæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º'})
        
        toml_path = Path(toml_path)
        if not toml_path.exists():
            return jsonify({'success': False, 'message': f'TOMLæ–‡ä»¶ä¸å­˜åœ¨: {toml_path}'})
        
        import toml
        with open(toml_path, 'r', encoding='utf-8') as f:
            toml_config = toml.load(f)
        
        batch_size = None
        
        # ä» [general] éƒ¨åˆ†è¯»å– batch_size
        if 'general' in toml_config and 'batch_size' in toml_config['general']:
            batch_size = toml_config['general']['batch_size']
        
        if batch_size is not None:
            log_message(f"ä» {toml_path} è¯»å– batch_size: {batch_size}", 'info')
            return jsonify({'success': True, 'batch_size': batch_size})
        else:
            return jsonify({'success': False, 'message': 'TOMLæ–‡ä»¶ä¸­æœªæ‰¾åˆ° batch_size å‚æ•°ï¼ˆæ£€æŸ¥äº† [general] éƒ¨åˆ†ï¼‰', 'batch_size': None})
    
    except Exception as e:
        log_message(f"è¯»å–TOMLæ–‡ä»¶å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'è¯»å–TOMLæ–‡ä»¶å¤±è´¥: {e}', 'batch_size': None})

@app.route('/api/read_num_repeats_from_toml', methods=['POST'])
def api_read_num_repeats_from_toml():
    """ä»TOMLæ–‡ä»¶è¯»å–num_repeats"""
    try:
        data = request.get_json()
        toml_path = data.get('toml_path')
        
        if not toml_path:
            return jsonify({'success': False, 'message': 'TOMLæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º'})
        
        toml_path = Path(toml_path)
        if not toml_path.exists():
            return jsonify({'success': False, 'message': f'TOMLæ–‡ä»¶ä¸å­˜åœ¨: {toml_path}'})
        
        import toml
        with open(toml_path, 'r', encoding='utf-8') as f:
            toml_config = toml.load(f)
        
        num_repeats = None
        
        # é¦–å…ˆæ£€æŸ¥ [general] éƒ¨åˆ†
        if 'general' in toml_config and 'num_repeats' in toml_config['general']:
            num_repeats = toml_config['general']['num_repeats']
        
        # å¦‚æœ [general] ä¸­æ²¡æœ‰ï¼Œæ£€æŸ¥ [[datasets]] éƒ¨åˆ†
        if num_repeats is None and 'datasets' in toml_config:
            for dataset in toml_config['datasets']:
                if 'num_repeats' in dataset:
                    num_repeats = dataset['num_repeats']
                    break  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ num_repeats
        
        if num_repeats is not None:
            log_message(f"ä» {toml_path} è¯»å– num_repeats: {num_repeats}", 'info')
            return jsonify({'success': True, 'num_repeats': num_repeats})
        else:
            return jsonify({'success': False, 'message': 'TOMLæ–‡ä»¶ä¸­æœªæ‰¾åˆ° num_repeats å‚æ•°ï¼ˆæ£€æŸ¥äº† [general] å’Œ [[datasets]] éƒ¨åˆ†ï¼‰', 'num_repeats': None})
    
    except Exception as e:
        log_message(f"è¯»å–TOMLæ–‡ä»¶å¤±è´¥: {e}", 'error')
        return jsonify({'success': False, 'message': f'è¯»å–TOMLæ–‡ä»¶å¤±è´¥: {e}', 'num_repeats': None})

def start_tensorboard_process():
    """ä»…å¯åŠ¨TensorBoardè¿›ç¨‹ï¼Œä¸æ‰“å¼€æµè§ˆå™¨"""
    def run_tensorboard():
        try:
            # æ£€æŸ¥TensorBoardæ˜¯å¦å·²ç»åœ¨è¿è¡Œ
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', 6006))
            sock.close()
            
            if result != 0:  # ç«¯å£æœªè¢«å ç”¨ï¼Œå¯åŠ¨TensorBoard
                log_message("å¯åŠ¨TensorBoardè¿›ç¨‹...", 'info')
                
                # è¯»å–è™šæ‹Ÿç¯å¢ƒé…ç½®
                config_file = 'webui_config.json'
                config = {}
                if os.path.exists(config_file):
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                    except Exception as e:
                        log_message(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}", 'warning')
                
                # è·å–è™šæ‹Ÿç¯å¢ƒè®¾ç½®
                enable_venv = config.get('enable_venv', True)
                venv_python_path = config.get('venv_python_path', './venv/Scripts/')
                
                if enable_venv:
                    # æ„å»ºè™šæ‹Ÿç¯å¢ƒPythonè·¯å¾„
                    if not venv_python_path.endswith(('/', '\\')):
                        venv_python_path += '/'
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    if venv_python_path.startswith('./'):
                        venv_python_path = venv_python_path[2:]  # ç§»é™¤ './'
                    venv_python = os.path.join(os.getcwd(), venv_python_path, 'python.exe')
                    venv_python = os.path.normpath(venv_python)
                    
                    log_message(f"ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒPythonå¯åŠ¨TensorBoard: {venv_python}", 'info')
                    python_cmd = venv_python
                else:
                    log_message("ä½¿ç”¨ç³»ç»ŸPythonå¯åŠ¨TensorBoard", 'info')
                    python_cmd = sys.executable
                
                subprocess.Popen([
                    python_cmd, "-m", "tensorboard.main", 
                    "--logdir", "./logs", "--port", "6006", "--host", "0.0.0.0"
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                # ç­‰å¾…TensorBoardå¯åŠ¨
                time.sleep(3)
                log_message("TensorBoardè¿›ç¨‹å·²å¯åŠ¨: http://localhost:6006", 'success')
            else:
                log_message("TensorBoardå·²åœ¨è¿è¡Œä¸­", 'info')
            
        except Exception as e:
            log_message(f"å¯åŠ¨TensorBoardå¤±è´¥: {e}", 'error')
    
    # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨TensorBoard
    threading.Thread(target=run_tensorboard, daemon=True).start()

def start_tensorboard():
    """å¯åŠ¨TensorBoardå¹¶æ‰“å¼€æµè§ˆå™¨"""
    def open_tensorboard():
        try:
            # æ£€æŸ¥TensorBoardæ˜¯å¦åœ¨è¿è¡Œ
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', 6006))
            sock.close()
            
            if result == 0:  # TensorBoardæ­£åœ¨è¿è¡Œ
                # ç›´æ¥æ‰“å¼€æµè§ˆå™¨
                webbrowser.open('http://localhost:6006')
                log_message("å·²æ‰“å¼€TensorBoardé¡µé¢: http://localhost:6006", 'success')
            else:
                # TensorBoardæœªè¿è¡Œï¼Œå…ˆå¯åŠ¨å†æ‰“å¼€
                start_tensorboard_process()
                time.sleep(3)  # ç­‰å¾…å¯åŠ¨
                webbrowser.open('http://localhost:6006')
                log_message("TensorBoardå·²å¯åŠ¨å¹¶æ‰“å¼€: http://localhost:6006", 'success')
            
        except Exception as e:
            log_message(f"æ‰“å¼€TensorBoardå¤±è´¥: {e}", 'error')
    
    # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†
    threading.Thread(target=open_tensorboard, daemon=True).start()

def clear_logs_directory():
    """æ¸…ç©ºlogsç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•"""
    logs_dir = Path('./logs')
    try:
        if logs_dir.exists():
            import shutil
            # åˆ é™¤æ•´ä¸ªlogsç›®å½•åŠå…¶å†…å®¹
            shutil.rmtree(logs_dir)
            log_message("å·²åˆ é™¤logsç›®å½•åŠå…¶æ‰€æœ‰å†…å®¹", 'info')
            # é‡æ–°åˆ›å»ºç©ºçš„logsç›®å½•
            logs_dir.mkdir(exist_ok=True)
            log_message("å·²é‡æ–°åˆ›å»ºlogsç›®å½•", 'info')
        else:
            # å¦‚æœlogsç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
            logs_dir.mkdir(exist_ok=True)
            log_message("å·²åˆ›å»ºlogsç›®å½•", 'info')
        log_message("logsç›®å½•å·²æ¸…ç©º", 'success')
    except Exception as e:
        log_message(f"æ¸…ç©ºlogsç›®å½•å¤±è´¥: {e}", 'error')

def signal_handler(sig, frame):
    """å¤„ç†Ctrl+Cä¿¡å·"""
    print("\næ­£åœ¨å…³é—­WebæœåŠ¡å™¨...")
    stop_current_process()
    sys.exit(0)

if __name__ == '__main__':
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æŠ‘åˆ¶MSVCè­¦å‘Š
    os.environ['DISTUTILS_USE_SDK'] = '1'
    os.environ['MSSdk'] = '1'
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    
    # ç¡®ä¿logsç›®å½•å­˜åœ¨ï¼ˆä¸æ¸…ç†ï¼‰
    logs_dir = Path('./logs')
    logs_dir.mkdir(exist_ok=True)
    
    # è‡ªåŠ¨å¯åŠ¨TensorBoardè¿›ç¨‹
    try:
        start_tensorboard_process()
        log_message("TensorBoardè¿›ç¨‹å·²è‡ªåŠ¨å¯åŠ¨", 'success')
    except Exception as e:
        log_message(f"TensorBoardè¿›ç¨‹è‡ªåŠ¨å¯åŠ¨å¤±è´¥: {e}", 'warning')
    
    print("ğŸš€ å¯åŠ¨ Qwen-Image Web UI")
    print("ğŸ“ è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ“Š TensorBoard: å·²è‡ªåŠ¨å¯åŠ¨ï¼Œç‚¹å‡»ç•Œé¢æŒ‰é’®æ‰“å¼€")
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    
    log_message("Web UI æœåŠ¡å™¨å¯åŠ¨", 'info')
    
    try:
        # ç¦ç”¨Flaskçš„è®¿é—®æ—¥å¿—
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.ERROR)
        
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        print(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")